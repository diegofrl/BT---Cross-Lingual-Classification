{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diegofrl/BT---Cross-Lingual-Classification/blob/main/POC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAg6BrjFY3ED"
      },
      "outputs": [],
      "source": [
        "!pip install gradio\n",
        "!pip install transformers\n",
        "!pip install gtts\n",
        "!pip install pydub\n",
        "!touch empty.mp3\n",
        "!git clone https://github.com/diegofrl/BT---Cross-Lingual-Classification.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import string\n",
        "from tensorflow import keras\n",
        "from keras.models import load_model\n",
        "import pickle\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-uncased', num_labels=4)\n",
        "model = AutoModel.from_pretrained('bert-base-multilingual-uncased', num_labels=4)\n",
        "\n",
        "def vectorize_text(text):\n",
        "    def is_subword(token):\n",
        "        return token[:2] == \"##\"\n",
        "\n",
        "    text = text.lower()\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        embeddings = model(torch.tensor([input_ids]))[0]\n",
        "\n",
        "    word_vectors = []\n",
        "    words = []\n",
        "    current_word = \"\"\n",
        "    current_word_vector = None\n",
        "\n",
        "    for i in range(1, len(tokens)-1):\n",
        "        token = tokens[i]\n",
        "        if not is_subword(token):\n",
        "            if current_word:\n",
        "                words.append(current_word)\n",
        "                word_vectors.append(current_word_vector)\n",
        "            current_word = token\n",
        "            current_word_vector = embeddings[0][i].numpy()\n",
        "        else:\n",
        "            current_word += token[2:]\n",
        "            current_word_vector += embeddings[0][i].numpy()\n",
        "\n",
        "    if current_word:\n",
        "        words.append(current_word)\n",
        "        word_vectors.append(current_word_vector)\n",
        "\n",
        "    df = pd.DataFrame({'word': words, 'vector': word_vectors})\n",
        "    return df\n",
        "\n",
        "\n",
        "def tf_get_predictions(text, model):\n",
        "    df = vectorize_text(text)\n",
        "    word_label_tuples = []\n",
        "    merged_tuples = []\n",
        "\n",
        "    for word, vector in zip(df['word'], df['vector']):\n",
        "        sample = np.array(vector).reshape(1, -1)\n",
        "        y_pred_one_hot = model.predict(sample, verbose=0)\n",
        "        label = np.argmax(y_pred_one_hot, axis=1)[0]\n",
        "        labels_str = {0: 'de', 1:'en', 2: 'fr', 3: 'it'}\n",
        "        label_str = labels_str.get(label)\n",
        "        word_label_tuples.append([word, label_str])\n",
        "\n",
        "    # Merge tuples of the same language\n",
        "    for word_label in word_label_tuples:\n",
        "        word, label = word_label\n",
        "        if not merged_tuples:\n",
        "            merged_tuples.append([word, label])\n",
        "        else:\n",
        "            last_tuple = merged_tuples[-1]\n",
        "            if label == last_tuple[1]:\n",
        "                last_tuple[0] += ' ' + word\n",
        "            else:\n",
        "                merged_tuples.append([word, label])\n",
        "\n",
        "    return merged_tuples\n",
        "\n",
        "# Load the neural network model\n",
        "nn_model = load_model('/content/BT---Cross-Lingual-Classification/ML_Models/transformer_nn_model.h5')\n",
        "\n",
        "text = \"How do you say 'Ich arbeite in Zürich' in English?\"\n",
        "predictions = tf_get_predictions(text, nn_model)\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "id": "HRpgWcLrOtYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gtts import gTTS\n",
        "from pydub import AudioSegment\n",
        "import tempfile\n",
        "\n",
        "def predictions_to_html(predictions):\n",
        "    html = \"<span style='color: #DB4437; font-size:16pt'>■</span> English <span style='color: #4285F4; font-size:16pt; margin-left:6pt'>■</span> French <span style='color: #F4B400; font-size:16pt; margin-left:6pt'>■</span> German  <span style='color: #0F9D58; font-size:16pt; margin-left:6pt'>■</span> Italian <br>\"\n",
        "    for prediction in predictions:\n",
        "      if prediction[1] == \"en\":\n",
        "        html += \"<span style='color: #DB4437'>\" #red\n",
        "      elif prediction[1] == \"fr\":\n",
        "        html += \"<span style='color: #4285F4'>\" #blue\n",
        "      elif prediction[1] == \"de\":\n",
        "        html += \"<span style='color: #F4B400'>\" #yellow\n",
        "      elif prediction[1] == \"it\":\n",
        "        html += \"<span style='color: #0F9D58'>\" #green\n",
        "      else:\n",
        "        html += \"<span>\"\n",
        "      html += prediction[0]+\"</span> \"\n",
        "    return html\n",
        "\n",
        "def crosslingual_tts(predictions, output_path):\n",
        "    # Initialize an empty audio segment\n",
        "    result_audio = AudioSegment.empty()\n",
        "\n",
        "    # Loop through the divs, extract text and language, and generate speech using gTTS\n",
        "    for prediction in predictions:\n",
        "        text = prediction[0]\n",
        "        lang = prediction[1]\n",
        "\n",
        "        # Convert text to speech using gTTS\n",
        "        tts = gTTS(text=text, lang=lang)\n",
        "        with tempfile.NamedTemporaryFile(delete=True) as audio_buffer:\n",
        "            tts.save(audio_buffer.name)\n",
        "\n",
        "            # Load the audio segment and concatenate it to the result\n",
        "            audio_segment = AudioSegment.from_file(audio_buffer.name, format='mp3')\n",
        "            result_audio += audio_segment\n",
        "\n",
        "    # Save the result as an mp3 file\n",
        "    result_audio.export(output_path, format='mp3')\n",
        "\n",
        "\n",
        "print(predictions_to_html(predictions))"
      ],
      "metadata": {
        "id": "kFFimhPlq_Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from gradio import themes\n",
        "\n",
        "def gradio_call(input_text):\n",
        "    if not input_text or input_text == \"\":\n",
        "        return \"<span style='color: #DB4437; font-weight: bold;'>ERROR: Please provide an input text</span>\", \"empty.mp3\"\n",
        "    input_text = re.sub('[^\\w\\s]', '', input_text)\n",
        "    predictions = tf_get_predictions(input_text, nn_model)\n",
        "    crosslingual_tts(predictions, \"output.mp3\")\n",
        "    html = predictions_to_html(predictions)\n",
        "    return html, \"output.mp3\"\n",
        "\n",
        "theme = gr.themes.Base(\n",
        "    primary_hue=gr.themes.colors.blue,\n",
        "    secondary_hue=gr.themes.colors.neutral,\n",
        "    neutral_hue=gr.themes.colors.neutral,\n",
        "    text_size=gr.themes.sizes.text_md,\n",
        "    radius_size=gr.themes.sizes.radius_lg,\n",
        "    font=[gr.themes.GoogleFont('Plus Jakarta Sans'), gr.themes.GoogleFont('Source Sans Pro'), 'ui-sans-serif', 'sans-serif'],\n",
        "    font_mono=[gr.themes.GoogleFont('Source Code Pro'), gr.themes.GoogleFont('IBM Plex Mono'), 'ui-monospace', 'Consolas'],\n",
        ")\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=gradio_call,\n",
        "    examples=[\"How do you say ich arbeite in Zürich in English?\", \"As we say in french: pierre qui roule n'amasse pas mousse!\",  \"I have attended a haute couture show last week in Paris.\"],\n",
        "    inputs=[\n",
        "        gr.components.Textbox(label=\"Input\", placeholder=\"Do you sprichst Français?\", lines=4),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.components.HTML(),\n",
        "        gr.components.Audio(label=\"Audio\", type=\"filepath\")\n",
        "    ],\n",
        "    title=\"Do you sprichst français?\",\n",
        "    description='''\n",
        "### This app can perform cross-lingual language detection and text-to-speech.\\n\n",
        "### Disclaimer:\\n\n",
        "It is built as a proof of concept, that uses machine learning. Specific training data has been created for this project, which might impact the performances with some text categories due to a limited amount of data. This might cause prediction errors when it encounters very short or unfamiliar inputs.\\n\n",
        "Examples that work well:\\n\n",
        "    \"How do you say ich arbeite in Zürich in English?\",\n",
        "    \"As we say in french: pierre qui roule n'amasse pas mousse!\",\n",
        "    \"I have attended a haute couture show last week in Paris.\"\n",
        "Examples that might not work:\\n\n",
        "    \"Danke!\",\n",
        "    \"Une thèse de Diego Fraile et Hugo Perotto.\",\n",
        "    \"Thx u 2!\"\n",
        "    ''',\n",
        "    theme=theme,\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "LMijL4UxY7qy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}